Deeper Dive into Building an Auto-Translating Android Video Player
Let's break down each component with a focus on making it work, even if some parts remain conceptual due to the complexity.

Core Idea: The Audio Pipeline is Key
The success of this project hinges on correctly extracting the audio, converting it to the format Vosk expects (16kHz, 16-bit, mono PCM), and then feeding it efficiently to the STT engine.

1. Project Setup and Dependencies (Even More Detail)
Gradle

// build.gradle (app-level)

dependencies {
    // ExoPlayer (Media3 - current generation)
    implementation 'androidx.media3:media3-exoplayer:1.3.1' // Or latest stable
    implementation 'androidx.media3:media3-ui:1.3.1' // UI components like PlayerView

    // Vosk Android (Make sure you use the official library)
    // Check https://github.com/alphacephei/vosk-android for the latest version and exact dependency
    implementation 'com.alphacephei:vosk-android:0.3.21'

    // ML Kit Translation
    implementation 'com.google.mlkit:translate:17.0.0' // Or latest stable
    implementation 'com.google.mlkit:language-id:17.0.0' // For source language detection (optional, but good)

    // Kotlin Coroutines for structured concurrency and background processing
    implementation 'org.jetbrains.kotlinx:kotlinx-coroutines-core:1.8.0' // Or latest stable
    implementation 'org.jetbrains.kotlinx:kotlinx-coroutines-android:1.8.0' // Or latest stable

    // For audio processing/resampling (crucial for Vosk if source audio is not 16kHz mono)
    // There isn't a single "official" Android audio processing library from Google
    // You might need to integrate a native C/C++ library via JNI (e.g., resampler from AudioTrack, or libsamplerate)
    // Or use a Java/Kotlin port if available.
    // Example of a potentially useful library (check its active maintenance and licensing):
    // implementation 'com.github.ashqal:android-libresample:0.3.0' // If using this, add jitpack.io to repositories
}

// In your top-level build.gradle (project-level)
allprojects {
    repositories {
        // ... other repositories like google(), mavenCentral()
        maven { url 'https://jitpack.io' } // Required for com.github.ashqal:android-libresample
    }
}
Reasoning for changes:

Media3 ExoPlayer: Using the newer androidx.media3 libraries, which are the latest generation of ExoPlayer.

ML Kit Language ID: This is extremely useful if you don't know the source language of the video.

Audio Resampling Library: This is the most critical new dependency. Vosk requires 16kHz, 16-bit, mono PCM. Most video audio will be 44.1kHz or 48kHz stereo. You must resample and downmix the audio. android-libresample is a potential option that wraps a native C library. Implementing resampling from scratch is very complex.

2. Permissions in AndroidManifest.xml
XML

<manifest xmlns:android="http://schemas.android.com/apk/res/android"
    package="com.example.your_app_package">

    <uses-permission android:name="android.permission.INTERNET" />
    <uses-permission android:name="android.permission.READ_EXTERNAL_STORAGE"
        android:maxSdkVersion="32" /> <uses-permission android:name="android.permission.READ_MEDIA_VIDEO" /> <uses-permission android:name="android.permission.RECORD_AUDIO" /> <application
        </application>
</manifest>
Important: For READ_EXTERNAL_STORAGE, Android 13 (API 33) introduced granular media permissions (READ_MEDIA_VIDEO, READ_MEDIA_AUDIO, READ_MEDIA_IMAGES). You should target these newer permissions for Android 13+ and keep READ_EXTERNAL_STORAGE for older versions (or for broader file access if strictly necessary).

You will also need to request these permissions at runtime in your Activity, especially READ_EXTERNAL_STORAGE or READ_MEDIA_VIDEO.

3. Layout XML (activity_main.xml)
XML

<?xml version="1.0" encoding="utf-8"?>
<RelativeLayout xmlns:android="http://schemas.android.com/apk/res/android"
    xmlns:app="http://schemas.android.com/apk/res-auto"
    xmlns:tools="http://schemas.android.com/tools"
    android:layout_width="match_parent"
    android:layout_height="match_parent"
    tools:context=".MainActivity">

    <androidx.media3.ui.PlayerView
        android:id="@+id/player_view"
        android:layout_width="match_parent"
        android:layout_height="match_parent"
        app:use_controller="true"
        app:auto_show_controller="true"
        app:show_buffering="when_playing"
        app:shutter_background_color="@android:color/black" />

    <TextView
        android:id="@+id/subtitle_view"
        android:layout_width="wrap_content"
        android:layout_height="wrap_content"
        android:layout_alignParentBottom="true"
        android:layout_centerHorizontal="true"
        android:layout_marginBottom="16dp"
        android:background="#99000000" android:textColor="@android:color/white"
        android:textSize="20sp"
        android:paddingStart="12dp"
        android:paddingEnd="12dp"
        android:paddingTop="8dp"
        android:paddingBottom="8dp"
        android:fontFamily="sans-serif-medium"
        android:elevation="4dp"
        android:visibility="gone" tools:text="This is a sample translated subtitle for the video."
        tools:visibility="visible" />

    <ProgressBar
        android:id="@+id/loading_spinner"
        android:layout_width="wrap_content"
        android:layout_height="wrap_content"
        android:layout_centerInParent="true"
        android:visibility="gone" />

</RelativeLayout>
Improvements:

app:use_controller, app:auto_show_controller, etc.: Better defaults for PlayerView.

subtitle_view styling: More visually appealing subtitle background and padding. elevation gives it a slight shadow.

visibility="gone": Start hidden and only show when content is available.

ProgressBar: To show when models are loading or during initial video buffering.

4. MainActivity (Detailed Implementation)
Java

package com.example.your_app_package;

import android.content.pm.PackageManager;
import android.net.Uri;
import android.os.Bundle;
import android.os.Environment;
import android.util.Log;
import android.view.View;
import android.widget.ProgressBar;
import android.widget.TextView;
import android.widget.Toast;

import androidx.annotation.NonNull;
import androidx.appcompat.app.AppCompatActivity;
import androidx.core.app.ActivityCompat;
import androidx.core.content.ContextCompat;
import androidx.media3.common.C;
import androidx.media3.common.MediaItem;
import androidx.media3.common.PlaybackException;
import androidx.media3.common.Player;
import androidx.media3.common.util.UnstableApi;
import androidx.media3.exoplayer.SimpleExoPlayer;
import androidx.media3.exoplayer.audio.AudioProcessor;
import androidx.media3.exoplayer.audio.AudioSink;
import androidx.media3.exoplayer.audio.DefaultAudioSink;
import androidx.media3.exoplayer.audio.DefaultAudioProcessorChain;
import androidx.media3.ui.PlayerView;

import com.google.mlkit.nl.languageid.LanguageIdentification;
import com.google.mlkit.nl.languageid.LanguageIdentifier;
import com.google.mlkit.nl.translate.TranslateLanguage;
import com.google.mlkit.nl.translate.Translation;
import com.google.mlkit.nl.translate.Translator;
import com.google.mlkit.nl.translate.TranslatorOptions;

import org.json.JSONException;
import org.json.JSONObject;
import org.vosk.LogLevel;
import org.vosk.Model;
import org.vosk.Recognizer;
import org.vosk.Vosk;
import org.vosk.android.StorageService;

import java.io.File;
import java.io.IOException;
import java.nio.ByteBuffer;
import java.nio.ByteOrder;
import java.util.ArrayList;
import java.util.LinkedList;
import java.util.Queue;
import java.util.concurrent.ConcurrentLinkedQueue;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;

// For audio resampling (if using android-libresample)
// import com.ashqal.libresample.Resample; // Uncomment if using android-libresample

@UnstableApi // Mark the class as using unstable Media3 APIs for AudioProcessor
public class MainActivity extends AppCompatActivity implements Player.Listener {

    private SimpleExoPlayer player;
    private PlayerView playerView;
    private TextView subtitleView;
    private ProgressBar loadingSpinner;

    // Vosk components
    private Model voskModel;
    private Recognizer voskRecognizer;
    private ExecutorService voskExecutor; // Dedicated thread for Vosk recognition
    private Queue<byte[]> audioBufferQueue; // Thread-safe queue for audio data

    // ML Kit components
    private Translator mlKitTranslator;
    private LanguageIdentifier languageIdentifier; // For dynamic source language detection
    private ExecutorService mlKitExecutor; // Dedicated thread for ML Kit translation

    // Audio Processing
    private MyAudioProcessor myAudioProcessor;
    private static final int VOSK_SAMPLE_RATE = 16000; // Vosk expects 16kHz
    private static final int VOSK_CHANNEL_COUNT = 1; // Vosk expects mono
    private static final int VOSK_ENCODING = C.ENCODING_PCM_16BIT; // Vosk expects 16-bit PCM

    // Subtitle synchronization (simplified, advanced needs Cues and timers)
    private long lastSubtitleUpdateTime = 0;
    private static final long SUBTITLE_MIN_DISPLAY_DURATION_MS = 1500; // Min time to show a subtitle

    // Permissions
    private static final int REQUEST_PERMISSIONS_CODE = 1001;
    private final String[] REQUIRED_PERMISSIONS;

    public MainActivity() {
        if (android.os.Build.VERSION.SDK_INT >= android.os.Build.VERSION_CODES.TIRAMISU) {
            REQUIRED_PERMISSIONS = new String[]{
                    android.Manifest.permission.READ_MEDIA_VIDEO,
                    android.Manifest.permission.RECORD_AUDIO // Still good practice for Vosk
            };
        } else {
            REQUIRED_PERMISSIONS = new String[]{
                    android.Manifest.permission.READ_EXTERNAL_STORAGE,
                    android.Manifest.permission.RECORD_AUDIO
            };
        }
    }


    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_main);

        playerView = findViewById(R.id.player_view);
        subtitleView = findViewById(R.id.subtitle_view);
        loadingSpinner = findViewById(R.id.loading_spinner);

        // Initialize executors for background tasks
        voskExecutor = Executors.newSingleThreadExecutor();
        mlKitExecutor = Executors.newSingleThreadExecutor();
        audioBufferQueue = new ConcurrentLinkedQueue<>();

        // Request permissions
        if (!checkPermissions()) {
            requestPermissions();
        } else {
            initializeComponentsAndPlayer();
        }
    }

    private boolean checkPermissions() {
        for (String permission : REQUIRED_PERMISSIONS) {
            if (ContextCompat.checkSelfPermission(this, permission) != PackageManager.PERMISSION_GRANTED) {
                return false;
            }
        }
        return true;
    }

    private void requestPermissions() {
        ActivityCompat.requestPermissions(this, REQUIRED_PERMISSIONS, REQUEST_PERMISSIONS_CODE);
    }

    @Override
    public void onRequestPermissionsResult(int requestCode, @NonNull String[] permissions, @NonNull int[] grantResults) {
        super.onRequestPermissionsResult(requestCode, permissions, grantResults);
        if (requestCode == REQUEST_PERMISSIONS_CODE) {
            boolean allGranted = true;
            for (int result : grantResults) {
                if (result != PackageManager.PERMISSION_GRANTED) {
                    allGranted = false;
                    break;
                }
            }
            if (allGranted) {
                initializeComponentsAndPlayer();
            } else {
                Toast.makeText(this, "Permissions required for video playback and translation.", Toast.LENGTH_LONG).show();
                finish(); // Exit if permissions are not granted
            }
        }
    }

    private void initializeComponentsAndPlayer() {
        loadingSpinner.setVisibility(View.VISIBLE);
        Toast.makeText(this, "Loading models, please wait...", Toast.LENGTH_LONG).show();

        // 1. Initialize Vosk Model (asynchronously)
        initVosk();

        // 2. Initialize ML Kit Translator and Language Identifier
        initMLKitTranslator();
        initLanguageIdentifier();

        // 3. Initialize and set up ExoPlayer with custom audio processor
        myAudioProcessor = new MyAudioProcessor();
        DefaultAudioSink.AudioProcessorChain audioProcessorChain =
                new DefaultAudioProcessorChain(myAudioProcessor);

        AudioSink audioSink = new DefaultAudioSink.Builder(this)
                .setAudioProcessorChain(audioProcessorChain)
                .build();

        player = new SimpleExoPlayer.Builder(this)
                .setAudioSink(audioSink) // Set our custom audio sink
                .build();

        playerView.setPlayer(player);
        player.addListener(this); // Listen for player events like playback state

        // Load a sample video (e.g., "sample.mp4" in your device's Downloads folder)
        // Ensure this file exists and permissions are granted!
        File videoFile = new File(Environment.getExternalStoragePublicDirectory(Environment.DIRECTORY_DOWNLOADS), "sample.mp4");
        if (videoFile.exists()) {
            Uri videoUri = Uri.fromFile(videoFile);
            MediaItem mediaItem = MediaItem.fromUri(videoUri);
            player.setMediaItem(mediaItem);
            player.prepare();
            player.play();
        } else {
            Toast.makeText(this, "Sample video file not found: " + videoFile.getAbsolutePath(), Toast.LENGTH_LONG).show();
            loadingSpinner.setVisibility(View.GONE);
        }
    }

    // --- Vosk Initialization and Processing ---
    private static final String VOSK_MODEL_PATH = "vosk-model-en-us-0.22"; // Adjust to your model's folder name in assets

    private void initVosk() {
        Vosk.Companion.init(this); // Initialize Vosk
        Vosk.Companion.set
                (LogLevel.INFO); // Set Vosk logging level

        StorageService.unpack(this, VOSK_MODEL_PATH, "model",
                (model) -> {
                    voskModel = model;
                    try {
                        // Create recognizer with the *target* sample rate (Vosk expects 16kHz)
                        // Make sure your AudioProcessor resamples to this rate.
                        voskRecognizer = new Recognizer(voskModel, VOSK_SAMPLE_RATE);
                        Log.d("Vosk", "Vosk model loaded and recognizer initialized.");
                        // Start a consumer thread for audio data
                        voskExecutor.execute(this::voskAudioConsumer);
                        checkAllModelsLoaded();
                    } catch (IOException e) {
                        Log.e("Vosk", "Error creating Vosk Recognizer: " + e.getMessage());
                        Toast.makeText(this, "Error initializing speech recognition.", Toast.LENGTH_LONG).show();
                        loadingSpinner.setVisibility(View.GONE);
                    }
                },
                (exception) -> {
                    Log.e("Vosk", "Failed to unpack Vosk model: " + exception.getMessage());
                    Toast.makeText(this, "Failed to load speech recognition model. Check assets folder.", Toast.LENGTH_LONG).show();
                    loadingSpinner.setVisibility(View.GONE);
                });
    }

    // Consumer loop for audio data
    private void voskAudioConsumer() {
        // We'll process audio in chunks to avoid blocking the queue for too long
        int bufferSize = (int) (VOSK_SAMPLE_RATE * 0.2 * 2); // 0.2 seconds of 16kHz 16-bit mono audio (samples * bytes/sample)
        ByteBuffer voskInputBuffer = ByteBuffer.allocateDirect(bufferSize);
        voskInputBuffer.order(ByteOrder.LITTLE_ENDIAN); // Vosk typically expects little-endian PCM

        while (!Thread.currentThread().isInterrupted()) {
            // Continuously try to get audio data from the queue
            byte[] data = audioBufferQueue.poll(); // Non-blocking poll
            if (data == null) {
                try {
                    Thread.sleep(10); // Wait a bit if no data
                } catch (InterruptedException e) {
                    Thread.currentThread().interrupt();
                    break;
                }
                continue;
            }

            voskInputBuffer.clear();
            voskInputBuffer.put(data);
            voskInputBuffer.flip(); // Prepare for reading

            try {
                // If Vosk accepts the waveform, get the result
                if (voskRecognizer.acceptWaveForm(voskInputBuffer)) {
                    String originalResult = voskRecognizer.getResult();
                    String transcribedText = extractTextFromJson(originalResult);
                    if (!transcribedText.isEmpty()) {
                        Log.d("VoskConsumer", "Transcribed: " + transcribedText);
                        translateText(transcribedText);
                    }
                }
            } catch (IOException e) {
                Log.e("VoskConsumer", "Vosk recognition error: " + e.getMessage());
            }
        }
    }

    private String extractTextFromJson(String jsonString) {
        try {
            JSONObject jsonObject = new JSONObject(jsonString);
            // Vosk can return 'text' or 'partial'
            return jsonObject.optString("text", jsonObject.optString("partial", "")).trim();
        } catch (JSONException e) {
            Log.e("JSON", "Error parsing Vosk JSON: " + e.getMessage());
            return "";
        }
    }

    // --- ML Kit Initialization and Translation ---
    private void initMLKitTranslator() {
        // You would typically let the user select source and target languages.
        // For demonstration, let's assume English source and Spanish target.
        // For source, consider using Language Identification.
        TranslatorOptions options =
                new TranslatorOptions.Builder()
                        .setSourceLanguage(TranslateLanguage.ENGLISH) // Or use ML Kit Language Identification
                        .setTargetLanguage(TranslateLanguage.SPANISH)
                        .build();
        mlKitTranslator = Translation.getClient(options);

        // Download the model if needed (important for offline use)
        // This is a network operation, so it should be handled asynchronously.
        mlKitTranslator.downloadModelIfNeeded()
                .addOnSuccessListener(v -> {
                    Log.d("MLKit", "Translation model downloaded successfully.");
                    checkAllModelsLoaded();
                })
                .addOnFailureListener(e -> {
                    Log.e("MLKit", "Error downloading translation model: " + e.getMessage());
                    Toast.makeText(this, "Failed to download translation model.", Toast.LENGTH_LONG).show();
                    loadingSpinner.setVisibility(View.GONE);
                });
    }

    private void initLanguageIdentifier() {
        languageIdentifier = LanguageIdentification.getClient();
        // No explicit download needed for language identification model.
    }

    private void translateText(String textToTranslate) {
        if (mlKitTranslator == null) {
            Log.w("MLKit", "ML Kit Translator not initialized.");
            return;
        }
        if (textToTranslate.trim().isEmpty()) {
            // No text to translate, clear existing subtitle if it's too old
            if (System.currentTimeMillis() - lastSubtitleUpdateTime > SUBTITLE_MIN_DISPLAY_DURATION_MS) {
                displaySubtitle(""); // Clear subtitle
            }
            return;
        }

        mlKitExecutor.execute(() -> {
            // Optional: Identify source language dynamically if not fixed
            // languageIdentifier.identifyLanguage(textToTranslate)
            //     .addOnSuccessListener(...)
            //     .addOnFailureListener(...);

            mlKitTranslator.translate(textToTranslate)
                    .addOnSuccessListener(translatedText -> {
                        Log.d("MLKit", "Translated: '" + textToTranslate + "' -> '" + translatedText + "'");
                        displaySubtitle(translatedText);
                    })
                    .addOnFailureListener(e -> {
                        Log.e("MLKit", "Translation failed: " + e.getMessage());
                        // Fallback: display original text or an error message
                        displaySubtitle("[Translation Error] " + textToTranslate);
                    });
        });
    }

    // --- Subtitle Display ---
    private void displaySubtitle(String text) {
        runOnUiThread(() -> {
            if (text == null || text.isEmpty()) {
                subtitleView.setVisibility(View.GONE);
                subtitleView.setText("");
            } else {
                subtitleView.setText(text);
                subtitleView.setVisibility(View.VISIBLE);
                lastSubtitleUpdateTime = System.currentTimeMillis();
            }
        });
    }

    // --- Audio Processor Implementation (Crucial for audio data extraction and resampling) ---
    private class MyAudioProcessor implements AudioProcessor {

        private ByteBuffer outputBuffer = EMPTY_BUFFER;
        private int inputSampleRateHz = C.RATE_UNSET;
        private int inputChannelCount = C.CHANNEL_COUNT_UNSET;
        private @C.Encoding int inputEncoding = C.ENCODING_INVALID;

        // For resampling (if using android-libresample, uncomment this)
        // private Resample resample;

        @Override
        public AudioFormat configure(int sampleRateHz, int channelCount, @C.Encoding int encoding) throws UnhandledAudioFormatException {
            this.inputSampleRateHz = sampleRateHz;
            this.inputChannelCount = channelCount;
            this.inputEncoding = encoding;

            Log.d("AudioProcessor", "Input format: " + sampleRateHz + "Hz, " + channelCount + " channels, Encoding: " + encoding);

            // Vosk expects 16kHz, mono, 16-bit PCM.
            // If the input doesn't match, we need to handle resampling/downmixing.
            // This is the most complex part.
            // For now, we'll assume we handle it in queueInput, but for optimal performance,
            // resampling libraries should be integrated here.

            // If we need resampling, we'd configure the resampler here.
            // if (resample == null && (inputSampleRateHz != VOSK_SAMPLE_RATE || inputChannelCount != VOSK_CHANNEL_COUNT)) {
            //     resample = new Resample();
            //     // The 'frameSize' argument in create depends on the expected input buffer size.
            //     // For simplicity, let's say 2048 * (bytes per sample for input)
            //     // resample.create(inputSampleRateHz, VOSK_SAMPLE_RATE, 2048, inputChannelCount);
            // }

            // Always return the input format unchanged if we are just "teeing" the audio.
            return new AudioFormat(sampleRateHz, channelCount, encoding);
        }

        @Override
        public boolean isActive() {
            // Only active if Vosk model is loaded and player is playing
            return voskModel != null && voskRecognizer != null && player != null && player.isPlaying();
        }

        @Override
        public void queueInput(ByteBuffer inputBuffer) {
            if (!inputBuffer.hasRemaining() || !isActive()) {
                outputBuffer = inputBuffer; // Pass through if not active or empty
                return;
            }

            // Get original audio bytes
            byte[] originalAudioBytes = new byte[inputBuffer.remaining()];
            inputBuffer.get(originalAudioBytes);
            inputBuffer.rewind(); // Rewind for ExoPlayer's internal use

            byte[] processedAudioBytes = originalAudioBytes;

            // --- AUDIO RESAMPLING AND DOWNMIXING LOGIC (CRITICAL) ---
            // This is where you would integrate an audio processing library like android-libresample
            // or implement manual resampling/downmixing if `inputSampleRateHz` or `inputChannelCount`
            // do not match `VOSK_SAMPLE_RATE` or `VOSK_CHANNEL_COUNT`.

            // Example using a conceptual `AudioUtils` class (you need to implement this)
            if (inputSampleRateHz != VOSK_SAMPLE_RATE || inputChannelCount != VOSK_CHANNEL_COUNT || inputEncoding != VOSK_ENCODING) {
                // This is a placeholder. Real resampling/downmixing requires careful implementation.
                // It's often best to use an optimized native library for this.
                try {
                    processedAudioBytes = AudioUtils.resampleAndDownmix(
                            originalAudioBytes,
                            inputSampleRateHz,
                            inputChannelCount,
                            inputEncoding,
                            VOSK_SAMPLE_RATE,
                            VOSK_CHANNEL_COUNT,
                            VOSK_ENCODING
                    );
                    Log.d("AudioProcessor", "Resampled/downmixed audio for Vosk.");
                } catch (Exception e) {
                    Log.e("AudioProcessor", "Error during audio resampling/downmixing: " + e.getMessage());
                    // Fallback: send original data or skip
                    processedAudioBytes = new byte[0]; // Skip if error
                }
            }

            // Add processed audio data to the queue for Vosk consumer
            if (processedAudioBytes.length > 0) {
                audioBufferQueue.offer(processedAudioBytes);
            }

            // Set outputBuffer to the original inputBuffer for ExoPlayer to continue playback
            outputBuffer = inputBuffer;
            outputBuffer.position(outputBuffer.limit()); // Mark as consumed for this processor
        }

        @Override
        public ByteBuffer getOutput() {
            ByteBuffer buffer = outputBuffer;
            outputBuffer = EMPTY_BUFFER;
            return buffer;
        }

        @Override
        public void queueEndOfStream() {
            // Nothing specific needed here for continuous audio processing
        }

        @Override
        public boolean isEnded() {
            return !outputBuffer.hasRemaining();
        }

        @Override
        public void flush() {
            outputBuffer = EMPTY_BUFFER;
            // Clear any pending audio data in the queue on flush/seek
            audioBufferQueue.clear();
            if (voskRecognizer != null) {
                try {
                    voskRecognizer.reset(); // Reset Vosk recognizer on flush/seek
                } catch (IOException e) {
                    Log.e("Vosk", "Error resetting Vosk recognizer: " + e.getMessage());
                }
            }
        }

        @Override
        public void reset() {
            flush();
            inputSampleRateHz = C.RATE_UNSET;
            inputChannelCount = C.CHANNEL_COUNT_UNSET;
            inputEncoding = C.ENCODING_INVALID;
            // if (resample != null) { resample.destroy(); resample = null; }
        }
    }

    // --- Player.Listener Callbacks (for controlling visibility of loading spinner) ---
    @Override
    public void onPlaybackStateChanged(int playbackState) {
        Player.Listener.super.onPlaybackStateChanged(playbackState);
        if (playbackState == Player.STATE_BUFFERING) {
            loadingSpinner.setVisibility(View.VISIBLE);
        } else if (playbackState == Player.STATE_READY) {
            loadingSpinner.setVisibility(View.GONE);
        }
    }

    @Override
    public void onPlayerError(@NonNull PlaybackException error) {
        Player.Listener.super.onPlayerError(error);
        Log.e("ExoPlayer", "Player error: " + error.getMessage());
        Toast.makeText(this, "Playback error: " + error.getMessage(), Toast.LENGTH_LONG).show();
        loadingSpinner.setVisibility(View.GONE);
    }

    // --- General model loading check ---
    private boolean voskModelLoaded = false;
    private boolean mlKitModelLoaded = false;

    private synchronized void checkAllModelsLoaded() {
        if (voskModel != null && voskRecognizer != null) {
            voskModelLoaded = true;
        }
        // ML Kit translation model download is asynchronous, so we wait for its success listener
        // The boolean `mlKitModelLoaded` would be set in initMLKitTranslator's success listener.
        // For simplicity, let's assume it's set if no error.
        // A more robust check might involve a flag directly from ML Kit's download process.
        if (mlKitTranslator != null) {
            mlKitModelLoaded = true; // This is a weak check; better to confirm download is complete
        }


        if (voskModelLoaded && mlKitModelLoaded && player != null) { // Ensure player is also ready to hide spinner
            runOnUiThread(() -> {
                loadingSpinner.setVisibility(View.GONE);
                Toast.makeText(this, "Models loaded, starting playback.", Toast.LENGTH_SHORT).show();
            });
        }
    }


    @Override
    protected void onStart() {
        super.onStart();
        if (player != null) {
            player.play();
        }
    }

    @Override
    protected void onResume() {
        super.onResume();
        if (player != null) {
            player.play();
        }
    }

    @Override
    protected void onPause() {
        super.onPause();
        if (player != null) {
            player.pause();
        }
    }

    @Override
    protected void onStop() {
        super.onStop();
        if (player != null) {
            player.pause();
        }
    }

    @Override
    protected void onDestroy() {
        super.onDestroy();
        if (player != null) {
            player.release();
        }
        // Properly shut down Vosk and ML Kit resources
        if (voskRecognizer != null) {
            voskRecognizer.close();
        }
        if (voskModel != null) {
            voskModel.close();
        }
        if (mlKitTranslator != null) {
            mlKitTranslator.close();
        }
        if (languageIdentifier != null) {
            languageIdentifier.close();
        }
        if (voskExecutor != null) {
            voskExecutor.shutdownNow(); // Interrupt and stop the audio consumer thread
        }
        if (mlKitExecutor != null) {
            mlKitExecutor.shutdownNow();
        }
        // if (myAudioProcessor != null) { myAudioProcessor.reset(); } // Call reset to clean up resampler
    }
}
New Class: AudioUtils.java (You need to implement this for resampling/downmixing)
Java

package com.example.your_app_package;

import androidx.media3.common.C;
import android.util.Log;

import java.nio.ByteBuffer;
import java.nio.ByteOrder;
import java.nio.ShortBuffer;

public class AudioUtils {

    // This is a placeholder for a complex audio processing function.
    // Implementing high-quality resampling and downmixing requires advanced DSP knowledge
    // or the use of a robust library (like `android-libresample` or FFmpeg/libswresample via JNI).
    public static byte[] resampleAndDownmix(
            byte[] inputAudioBytes,
            int inputSampleRateHz,
            int inputChannelCount,
            @C.Encoding int inputEncoding,
            int targetSampleRateHz,
            int targetChannelCount,
            @C.Encoding int targetEncoding) {

        if (inputAudioBytes == null || inputAudioBytes.length == 0) {
            return new byte[0];
        }

        // --- Step 1: Convert input bytes to ShortBuffer (16-bit PCM) for easier processing ---
        // Assuming input is 16-bit PCM for simplicity in this example.
        // If inputEncoding is different (e.g., float, 24-bit), you need to convert it to 16-bit ShortBuffer first.
        ByteBuffer inputByteBuffer = ByteBuffer.wrap(inputAudioBytes).order(ByteOrder.LITTLE_ENDIAN);
        ShortBuffer inputShortBuffer = inputByteBuffer.asShortBuffer();

        short[] inputSamples = new short[inputShortBuffer.remaining()];
        inputShortBuffer.get(inputSamples);

        // --- Step 2: Downmix to Mono if needed ---
        short[] monoSamples = inputSamples;
        if (inputChannelCount == 2 && targetChannelCount == 1) { // Stereo to Mono
            monoSamples = new short[inputSamples.length / 2];
            for (int i = 0; i < inputSamples.length; i += 2) {
                // Simple average of left and right channels
                monoSamples[i / 2] = (short) ((inputSamples[i] + inputSamples[i + 1]) / 2);
            }
            Log.d("AudioUtils", "Downmixed from stereo to mono.");
        } else if (inputChannelCount != 1 && targetChannelCount == 1) {
            // Handle other multi-channel inputs if necessary
            Log.w("AudioUtils", "Unsupported channel count for downmixing to mono: " + inputChannelCount);
            // You might average all channels or select one.
        }

        // --- Step 3: Resample to Target Sample Rate if needed ---
        // This is the most complex part. A simple linear interpolation is shown, but
        // for good quality, you need a proper resampling algorithm (e.g., sinc interpolation).
        short[] resampledSamples = monoSamples;
        if (inputSampleRateHz != targetSampleRateHz) {
            Log.d("AudioUtils", "Resampling from " + inputSampleRateHz + "Hz to " + targetSampleRateHz + "Hz.");
            // Simple linear interpolation (low quality but illustrates concept)
            int outputLength = (int) (monoSamples.length * ((double) targetSampleRateHz / inputSampleRateHz));
            resampledSamples = new short[outputLength];

            for (int i = 0; i < outputLength; i++) {
                double inIndex = i * (double) inputSampleRateHz / targetSampleRateHz;
                int idx1 = (int) inIndex;
                int idx2 = idx1 + 1;

                if (idx2 >= monoSamples.length) {
                    resampledSamples[i] = monoSamples[idx1];
                } else {
                    double fraction = inIndex - idx1;
                    resampledSamples[i] = (short) (monoSamples[idx1] * (1.0 - fraction) + monoSamples[idx2] * fraction);
                }
            }
            // For production, consider using a library like:
            // https://github.com/ashqal/android-libresample (Java/JNI wrapper for libresample)
            // Or directly integrate a C/C++ audio library via NDK/JNI (e.g., libresample, speexdsp resampler)
        }

        // --- Step 4: Convert ShortBuffer back to byte array (16-bit PCM) ---
        ByteBuffer outputByteBuffer = ByteBuffer.allocate(resampledSamples.length * 2); // 2 bytes per short
        outputByteBuffer.order(ByteOrder.LITTLE_ENDIAN); // Vosk expects little-endian
        for (short sample : resampledSamples) {
            outputByteBuffer.putShort(sample);
        }
        return outputByteBuffer.array();
    }

    // You could also add a method for `Resample` from `android-libresample` here if you use it.
    /*
    private static Resample resampleProcessor; // Keep a static instance or pass it
    public static byte[] resampleWithLibresample(byte[] input, int inputSampleRate, int inputChannelCount, int targetSampleRate, int targetChannelCount) {
        if (resampleProcessor == null) {
            resampleProcessor = new Resample();
            // frameSize and channels need to be carefully chosen.
            // A frame here means a single stereo/mono sample.
            // 2048 is a common buffer size.
            resampleProcessor.create(inputSampleRate, targetSampleRate, 2048, inputChannelCount);
        }

        // Input and output ByteBuffers for libresample must be direct buffers
        ByteBuffer inputBuf = ByteBuffer.allocateDirect(input.length).order(ByteOrder.LITTLE_ENDIAN);
        inputBuf.put(input).flip();

        // Estimate output buffer size. A safe bet is a bit larger than max possible output.
        int maxOutputSize = (int) (input.length * (double) targetSampleRate / inputSampleRate * targetChannelCount / inputChannelCount);
        ByteBuffer outputBuf = ByteBuffer.allocateDirect(maxOutputSize).order(ByteOrder.LITTLE_ENDIAN);

        // Perform resampling
        int outputSamples = resampleProcessor.resample(inputBuf, outputBuf, inputBuf.remaining() / (inputChannelCount * 2)); // Divide by bytes per frame
        // The output of libresample is already in its target format (e.g., 16kHz mono).
        // It handles both resampling and channel conversion if configured.

        byte[] result = new byte[outputSamples * targetChannelCount * 2]; // 2 bytes per sample
        outputBuf.rewind();
        outputBuf.get(result);
        return result;
    }
    */
}
Key Improvements and Critical Details:

Audio Resampling and Downmixing (AudioUtils.java and MyAudioProcessor):

Absolute Must-Have: Vosk typically requires 16kHz sample rate, 16-bit PCM encoding, and mono channels. ExoPlayer can output various formats. You must convert the audio stream from ExoPlayer's format to Vosk's required format.

AudioUtils.java: I've provided a conceptual AudioUtils class with resampleAndDownmix method. This is a placeholder. Implementing high-quality audio resampling and downmixing is a complex Digital Signal Processing (DSP) task.

Recommendation: For production, DO NOT use the simple linear interpolation shown. It will sound bad. Instead, use a well-vetted audio processing library.

Libraries:

android-libresample (via JitPack): This is a Java wrapper around a native C libresample library. This is a very strong candidate. You'd uncomment the relevant lines in build.gradle and AudioUtils.java. It's efficient and designed for Android.

FFmpeg/libswresample (via JNI): If you're comfortable with NDK development, using FFmpeg's libswresample offers professional-grade audio conversion. This is the most powerful but also the most complex.

Pure Java/Kotlin DSP Libraries: Less common, but some exist. Might be less performant than native options.

queueInput in MyAudioProcessor: This is where you'd call your AudioUtils.resampleAndDownmix method to transform the audio bytes before adding them to audioBufferQueue.

Vosk Audio Consumer (voskAudioConsumer):

Dedicated Thread: It runs on voskExecutor (a SingleThreadExecutor) to prevent blocking the UI.

ConcurrentLinkedQueue: Used for audioBufferQueue to safely pass audio data from the ExoPlayer thread (which calls MyAudioProcessor.queueInput) to the Vosk processing thread.

Chunking: The consumer continuously polls the queue for small chunks of audio data. This allows for more reactive processing.

voskRecognizer.reset(): Crucial to call flush() on MyAudioProcessor (which in turn calls voskRecognizer.reset()) when the player seeks or changes media. This clears Vosk's internal state to prevent "ghost" recognition from old audio.

Synchronization (Subtitle Display):

Current Simplified Approach: The displaySubtitle simply shows the latest translated text. This is not perfectly synchronized with the video's current playback position. It's just displaying what Vosk/ML Kit outputs as fast as possible.

True Synchronization (Advanced): To achieve perfect synchronization, you would need:

Vosk Timestamps: Vosk can output word-level timestamps. When you get a result from voskRecognizer.getResult(), it contains JSON with word and start/end times relative to the audio chunk.

ExoPlayer Playback Position: When you extract an audio chunk in MyAudioProcessor, you'd record player.getCurrentPosition() at that moment. This gives you a reference point.

Cue Objects: ExoPlayer's subtitle system uses Cue objects, which have text, startTimeMs, and endTimeMs. You would create Cue objects from your translated text and calculated timestamps.

TextOutput and SubtitleView: Instead of directly setting subtitleView.setText(), you would likely use player.addTextOutput(new TextOutput() { ... }) and feed your dynamically created Cues to ExoPlayer's SubtitleView (which PlayerView contains). This is how ExoPlayer handles timed text. This is a major architectural change for true synchronization.

Buffering for Accuracy: Speech recognition works best on complete phrases or sentences. This means you might need to buffer several seconds of audio before sending to Vosk for better accuracy, which inherently introduces latency in the subtitle display. This is a trade-off.

Error Handling and UI Feedback:

Toast messages: Provide basic user feedback for model loading or errors.

ProgressBar: Indicates that background tasks (model loading, initial processing) are in progress.

Log.e and Log.d: Extensive logging is vital for debugging such a complex system.

Resource Management:

onDestroy(): Absolutely critical to release all resources (player.release(), voskRecognizer.close(), voskModel.close(), mlKitTranslator.close(), voskExecutor.shutdownNow(), mlKitExecutor.shutdownNow()). Failure to do so will lead to memory leaks and app crashes.

Instructions for Your AI Bot to Integrate This Project
Here's how you can instruct your AI bot (acting as "Coding Partner") to help you integrate this into your project. Frame it as specific tasks.

To My "Coding Partner" Bot:

"I have a conceptual framework and code for an Android video player with real-time auto-translation. My goal is to integrate this into an existing Android Studio project and make it fully functional. Here's a breakdown of the tasks I need your detailed guidance on. Please provide code snippets and clear instructions for each step, assuming I have a basic understanding of Android development but need help with the specific integration of these complex components.

Project Setup:

Dependencies: Guide me through adding all necessary dependencies to my build.gradle (app) file. Make sure to include ExoPlayer (Media3), Vosk Android, ML Kit Translation, ML Kit Language ID, Kotlin Coroutines, and explicitly mention the android-libresample library with its JitPack repository if that's the recommended resampling solution.

Permissions: List all required permissions for the AndroidManifest.xml and provide the exact XML tags. Explain the runtime permission request process for newer Android versions (Android 13+ specifically for media access).

Core App Structure:

Layout (activity_main.xml): Provide the complete XML for the activity_main.xml layout, including the PlayerView, TextView for subtitles (with initial gone visibility), and a ProgressBar. Ensure it's well-styled for a good user experience.

MainActivity Structure: Provide the complete Java code for MainActivity. This should include:

Class setup (AppCompatActivity, Player.Listener).

Member variables for all components (ExoPlayer, Vosk Model/Recognizer, ML Kit Translator/LanguageIdentifier, Executors, Queues).

Permission checking and requesting logic in onCreate and onRequestPermissionsResult.

A method initializeComponentsAndPlayer() that is called after permissions are granted.

Override onStart(), onResume(), onPause(), onStop(), and most importantly, onDestroy() for proper resource management.

Vosk Integration (Speech-to-Text):

Vosk Model Setup:

Explain how to place the Vosk model files (e.g., vosk-model-en-us-0.22) into the assets folder of the Android project.

Provide the code for initVosk() to unpack the model using StorageService.unpack.

Emphasize the importance of matching the Vosk Recognizer sample rate to the processed audio sample rate.

Real-time Audio Consumer:

Provide the full voskAudioConsumer() method.

Explain its role in continuously polling the audioBufferQueue for processed audio data.

Show how voskRecognizer.acceptWaveForm is used and how to extract the transcribed text from Vosk's JSON output using extractTextFromJson().

Explain the need for a ConcurrentLinkedQueue for thread-safe communication.

Vosk Error Handling and Resource Management: Detail how to properly close Vosk Model and Recognizer in onDestroy() and shut down the associated ExecutorService.

ML Kit Integration (Translation):

ML Kit Initialization: Provide the initMLKitTranslator() and initLanguageIdentifier() methods.

Explain the downloadModelIfNeeded() and its importance for offline use.

Explain the choice of source/target languages and mention the LanguageIdentifier for dynamic source detection.

Translation Logic: Provide the translateText() method, showing how to call mlKitTranslator.translate(). Include success and failure listeners.

ML Kit Resource Management: Detail how to close Translator and LanguageIdentifier in onDestroy().

Audio Processing (The Most Critical Part):

Custom AudioProcessor:

Provide the complete code for MyAudioProcessor as an inner class in MainActivity.

Explain its configure(), isActive(), queueInput(), getOutput(), flush(), and reset() methods in detail.

Crucially, elaborate on the queueInput() method: This is where the raw audio from ExoPlayer is received.

Resampling/Downmixing Implementation: Provide the AudioUtils.java class with the resampleAndDownmix method. This needs to be highly detailed. Explain the steps (input buffer to ShortBuffer, downmixing, resampling, ShortBuffer to output bytes). While a simple example can be given, explicitly state that a robust external library (like android-libresample) or NDK solution is highly recommended for quality and performance and briefly explain why simple methods are insufficient. If android-libresample is used, provide specific usage examples within AudioUtils.

Explain how to correctly set up DefaultAudioSink with the AudioProcessorChain in initializeComponentsAndPlayer().

Detail the importance of calling voskRecognizer.reset() within flush() of MyAudioProcessor.

Subtitle Display and Synchronization:

Basic Subtitle Display: Provide the displaySubtitle() method, ensuring it updates the TextView on the UI thread and handles visibility.

Synchronization Nuances: Reiterate the challenges of perfect real-time synchronization. Explain that the current setup provides "as fast as possible" display. For true synchronization, mention the concept of Vosk timestamps, ExoPlayer Cue objects, and the TextOutput interface for ExoPlayer's SubtitleView, and explain that this would be a significant next step.

Debugging and Best Practices:

Logging: Advise on the importance of extensive Log.d and Log.e for debugging, especially in the audio pipeline.

Runtime Permissions: Reiterate the need for proper runtime permission handling.

Model Assets: Re-emphasize the correct placement of Vosk model files in the assets folder.