package com.astralplayer.nextplayer.feature.ai

import android.content.Context
import android.graphics.Bitmap
import android.media.MediaMetadataRetriever
import android.net.Uri
import android.speech.RecognizerIntent
import android.speech.SpeechRecognizer
import android.util.Log
import com.google.mlkit.nl.translate.TranslateLanguage
import com.google.mlkit.nl.translate.Translation
import com.google.mlkit.nl.translate.Translator
import com.google.mlkit.nl.translate.TranslatorOptions
import com.google.mlkit.vision.common.InputImage
import com.google.mlkit.vision.label.ImageLabeling
import com.google.mlkit.vision.label.defaults.ImageLabelerOptions
import com.google.mlkit.vision.text.TextRecognition
import com.google.mlkit.vision.text.latin.TextRecognizerOptions
import kotlinx.coroutines.*
import kotlinx.coroutines.flow.MutableStateFlow
import kotlinx.coroutines.flow.StateFlow
import kotlinx.coroutines.flow.asStateFlow
import java.util.concurrent.TimeUnit
import kotlin.coroutines.resume
import kotlin.coroutines.suspendCoroutine

/**
 * Real implementation of video content analysis using ML Kit
 */
class VideoContentAnalyzer(private val context: Context) {
    
    private val claudeAI = ClaudeAIService(context)
    
    companion object {
        private const val TAG = "VideoContentAnalyzer"
        private const val ANALYSIS_FRAME_INTERVAL = 5000L // 5 seconds
        private const val MAX_FRAMES_TO_ANALYZE = 20
    }
    
    private val imageLabeler = ImageLabeling.getClient(
        ImageLabelerOptions.Builder()
            .setConfidenceThreshold(0.7f)
            .build()
    )
    
    private val textRecognizer = TextRecognition.getClient(TextRecognizerOptions.DEFAULT_OPTIONS)
    
    private val _analysisProgress = MutableStateFlow(0f)
    val analysisProgress: StateFlow<Float> = _analysisProgress.asStateFlow()
    
    private val _isAnalyzing = MutableStateFlow(false)
    val isAnalyzing: StateFlow<Boolean> = _isAnalyzing.asStateFlow()
    
    /**
     * Analyze video content and generate a summary
     */
    suspend fun analyzeVideoContent(videoUri: Uri): VideoAnalysisResult = withContext(Dispatchers.IO) {
        _isAnalyzing.value = true
        _analysisProgress.value = 0f
        
        val retriever = MediaMetadataRetriever()
        val detectedLabels = mutableMapOf<String, Int>()
        val detectedText = mutableSetOf<String>()
        val keyMoments = mutableListOf<KeyMoment>()
        
        try {
            retriever.setDataSource(context, videoUri)
            val durationStr = retriever.extractMetadata(MediaMetadataRetriever.METADATA_KEY_DURATION)
            val duration = durationStr?.toLongOrNull() ?: 0L
            
            val frameCount = minOf(
                (duration / ANALYSIS_FRAME_INTERVAL).toInt(),
                MAX_FRAMES_TO_ANALYZE
            )
            
            for (i in 0 until frameCount) {
                val timeMs = i * ANALYSIS_FRAME_INTERVAL
                val frame = retriever.getFrameAtTime(timeMs * 1000) ?: continue
                
                // Analyze frame for objects/scenes
                val labels = analyzeFrame(frame)
                labels.forEach { label ->
                    detectedLabels[label.text] = detectedLabels.getOrDefault(label.text, 0) + 1
                }
                
                // Extract text from frame
                val extractedText = extractTextFromFrame(frame)
                detectedText.addAll(extractedText)
                
                // Identify key moments based on label changes
                if (labels.isNotEmpty() && shouldMarkAsKeyMoment(labels, detectedLabels)) {
                    keyMoments.add(
                        KeyMoment(
                            timestamp = timeMs,
                            description = labels.joinToString { it.text },
                            thumbnail = frame
                        )
                    )
                }
                
                _analysisProgress.value = (i + 1).toFloat() / frameCount
            }
            
            retriever.release()
            
            // Generate basic summary
            val basicSummary = generateSummary(detectedLabels, detectedText, keyMoments, duration)
            
            // Enhance with Claude AI if available
            val enhancedSummary = try {
                if (AIServicesConfig.CLAUDE_API_KEY != "YOUR_CLAUDE_API_KEY") {
                    val videoDescription = "Video with detected objects: ${detectedLabels.keys.joinToString()}"
                    val analysisResult = claudeAI.analyzeVideoContent(
                        videoDescription = videoDescription,
                        detectedObjects = detectedLabels.keys.toList(),
                        extractedText = detectedText.toList(),
                        duration = duration
                    )
                    
                    buildString {
                        append(analysisResult.summary)
                        append("\n\n")
                        if (analysisResult.topics.isNotEmpty()) {
                            append("Topics: ${analysisResult.topics.joinToString(", ")}\n")
                        }
                        if (analysisResult.tags.isNotEmpty()) {
                            append("Tags: ${analysisResult.tags.joinToString(", ")}\n")
                        }
                        append("Rating: ${analysisResult.rating}\n")
                        if (analysisResult.insights.isNotEmpty()) {
                            append("\nInsights:\n")
                            analysisResult.insights.forEach { insight ->
                                append("â€¢ $insight\n")
                            }
                        }
                    }
                } else {
                    basicSummary
                }
            } catch (e: Exception) {
                Log.e(TAG, "Claude AI analysis failed, using basic summary", e)
                basicSummary
            }
            
            val summary = enhancedSummary
            
            _isAnalyzing.value = false
            
            VideoAnalysisResult(
                summary = summary,
                detectedObjects = detectedLabels.keys.toList(),
                extractedText = detectedText.toList(),
                keyMoments = keyMoments,
                duration = duration
            )
            
        } catch (e: Exception) {
            Log.e(TAG, "Error analyzing video", e)
            _isAnalyzing.value = false
            VideoAnalysisResult(
                summary = "Error analyzing video: ${e.message}",
                detectedObjects = emptyList(),
                extractedText = emptyList(),
                keyMoments = emptyList(),
                duration = 0L
            )
        }
    }
    
    /**
     * Analyze a single frame for labels
     */
    private suspend fun analyzeFrame(bitmap: Bitmap): List<ImageLabel> = suspendCoroutine { continuation ->
        val image = InputImage.fromBitmap(bitmap, 0)
        
        imageLabeler.process(image)
            .addOnSuccessListener { labels ->
                continuation.resume(labels.map { 
                    ImageLabel(it.text, it.confidence) 
                })
            }
            .addOnFailureListener { e ->
                Log.e(TAG, "Error labeling image", e)
                continuation.resume(emptyList())
            }
    }
    
    /**
     * Extract text from frame using OCR
     */
    private suspend fun extractTextFromFrame(bitmap: Bitmap): List<String> = suspendCoroutine { continuation ->
        val image = InputImage.fromBitmap(bitmap, 0)
        
        textRecognizer.process(image)
            .addOnSuccessListener { visionText ->
                val extractedText = visionText.textBlocks.map { it.text }
                continuation.resume(extractedText)
            }
            .addOnFailureListener { e ->
                Log.e(TAG, "Error extracting text", e)
                continuation.resume(emptyList())
            }
    }
    
    /**
     * Determine if current frame should be marked as key moment
     */
    private fun shouldMarkAsKeyMoment(
        currentLabels: List<ImageLabel>,
        allLabels: Map<String, Int>
    ): Boolean {
        // Mark as key moment if we see new objects or significant changes
        val newLabels = currentLabels.filter { label ->
            allLabels.getOrDefault(label.text, 0) <= 1
        }
        return newLabels.isNotEmpty() || currentLabels.size >= 3
    }
    
    /**
     * Generate human-readable summary
     */
    private fun generateSummary(
        detectedLabels: Map<String, Int>,
        detectedText: Set<String>,
        keyMoments: List<KeyMoment>,
        duration: Long
    ): String {
        val topLabels = detectedLabels.entries
            .sortedByDescending { it.value }
            .take(5)
            .map { it.key }
        
        val durationMinutes = TimeUnit.MILLISECONDS.toMinutes(duration)
        val durationSeconds = TimeUnit.MILLISECONDS.toSeconds(duration) % 60
        
        return buildString {
            append("Video Analysis Summary:\n\n")
            append("Duration: ${durationMinutes}m ${durationSeconds}s\n\n")
            
            if (topLabels.isNotEmpty()) {
                append("Main Content: ")
                append(topLabels.joinToString(", "))
                append("\n\n")
            }
            
            if (detectedText.isNotEmpty()) {
                append("Text Found: ")
                append(detectedText.take(3).joinToString(", "))
                if (detectedText.size > 3) {
                    append(" and ${detectedText.size - 3} more")
                }
                append("\n\n")
            }
            
            if (keyMoments.isNotEmpty()) {
                append("Key Moments: ${keyMoments.size} identified\n")
                keyMoments.take(3).forEach { moment ->
                    val time = TimeUnit.MILLISECONDS.toSeconds(moment.timestamp)
                    append("  - At ${time}s: ${moment.description.take(50)}\n")
                }
            }
        }
    }
    
    /**
     * Clean up resources
     */
    fun release() {
        imageLabeler.close()
        textRecognizer.close()
    }
    
    /**
     * Generate scene descriptions for accessibility
     */
    suspend fun generateAccessibilityDescriptions(keyMoments: List<KeyMoment>): Map<Long, String> {
        return try {
            val sceneData = keyMoments.map { moment ->
                SceneData(
                    timestamp = moment.timestamp,
                    objects = moment.description.split(", "),
                    action = "Scene at ${moment.timestamp / 1000}s"
                )
            }
            
            val descriptions = claudeAI.generateSceneDescriptions(sceneData)
            
            // Map descriptions to timestamps
            keyMoments.zip(descriptions).associate { (moment, desc) ->
                moment.timestamp to desc
            }
        } catch (e: Exception) {
            Log.e(TAG, "Failed to generate accessibility descriptions", e)
            emptyMap()
        }
    }
}

data class VideoAnalysisResult(
    val summary: String,
    val detectedObjects: List<String>,
    val extractedText: List<String>,
    val keyMoments: List<KeyMoment>,
    val duration: Long
)

data class KeyMoment(
    val timestamp: Long,
    val description: String,
    val thumbnail: Bitmap
)

data class ImageLabel(
    val text: String,
    val confidence: Float
)